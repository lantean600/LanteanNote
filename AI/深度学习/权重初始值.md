---
layout: default
title: "有理函数的积分"
mathjax: true
---

各层的激活值的分布都要求有适当的广度。通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。

## Xavier 初始值
主要用于 Sigmoid 或 Tanh函数（并且是浅层网络），实现了方差一致性，把权重控制在一个合理的范围内，让输出正好落在激活函数中间梯度最大的区域

```py
node_num = 100 # 前一层的节点数 
w = np.random.randn(node_num, node_num) / np.sqrt(node_num)
```

## He 初始值

主要用于ReLU函数且是深层网络，让每一层输出方差可以维持在合理的水平，既不会梯度消失也不会梯度爆炸。

```py
def he_init_normal(input_size, output_size):
    std = np.sqrt(2.0 / input_size)
    W = std * np.random.randn(input_size, output_size)
    return W
```

